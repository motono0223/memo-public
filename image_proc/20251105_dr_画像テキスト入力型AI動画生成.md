

# **画像・テキスト入力型AI動画生成プラットフォームに関する包括的デューデリジェンス・レポート**

## **セクション1: エグゼクティブ・サマリーと市場の二極化**

本レポートは、Nvidia Cosmosを起点に、「画像と自然言語（テキスト）の両方を入力として受け取る（Image-and-Text-to-Video、以下 I+T2V）」能力を持つ、主要なクラウドサービスおよび基盤モデルについて、包括的な技術的・戦略的デューデリジェンスを提供する。このI+T2V機能は、参照画像（被写体、スタイル、または初期フレーム）を提供し、テキストプロンプトでその動的な変化や文脈を指示するものであり、2025年現在の動画生成AIにおける最先端の技術標準を定義するものである。

調査の結果、I+T2V動画生成市場は、技術的基盤（Diffusion Transformer等）と入力モダリティ（I+T2V）において収斂の兆しを見せつつも、その**応用ドメイン**において明確に二極化していることが判明した。

1. フィジカルAI / 産業シミュレーション:  
   Nvidia (Cosmos) が主導するこのドメインは、自動運転車（AV）、ロボティクス、産業用デジタルツインのための物理的に正確な (physics-aware) 「World Foundation Models (WFMs)」の構築に焦点を当てている 1。ここでのI+T2Vは、特定のシナリオ（画像）における未来予測シミュレーション（テキストによる指示）のために使用される。  
2. クリエイティブAI / メディア生成:  
   OpenAI (Sora), Google (Veo), Meta (MovieGen), Adobe (Firefly) が主導するこのドメインは、映画制作、マーケティング、VFX、一般消費者向けコンテンツのための視覚的に説得力のある (photorealistic / cinematic) 動画生成に焦点を当てている 4。ここでのI+T2Vは、クリエイティブな制御（被写体の一貫性、スタイルの適用、動きの指示）のために使用される。

本レポートの分析によれば、ユーザーが提示したNvidia Cosmosは、後者のクリエイティブ・ドメインではなく、前者の産業シミュレーション・ドメインに特化したプラットフォームである。この認識のズレこそが、現在のAI動画生成市場の複雑性を象徴している。Cosmosは「映画のようなシーン」を作るツールではなく、自動運転AIのための「エッジケース訓練データ」を生成するプラットフォームである 1。

技術的な核心課題は、すべてのI+T2Vモデルに共通している。それは、参照画像が持つ「アイデンティティの一貫性維持」と、テキストプロンプトが要求する「編集・変化（動き）」という、本質的に相反する要求をいかにして両立させるかである。この「時間的・空間的一貫性 (Spatiotemporal Consistency)」 9 の維持能力こそが、SoraやVeoのような最先端のクローズドAPIと、Stable Video Diffusion (SVD) のような現行のオープンソースモデルを分ける決定的な技術的障壁となっている。

本レポートは、この「フィジカルAI vs. クリエイティブAI」という市場の二極化を前提に、まずベンチマークであるNvidia Cosmosの特異性を解明する（セクション2）。次に、クリエイティブ・ドメインにおける主要なクラウドAPI（Google Veo, OpenAI Sora）（セクション3）および商用SaaS（Adobe, Runway, Pika）（セクション4）の機能、API仕様、価格体系を徹底的に比較分析する。さらに、オープンソースモデルの現状とI+T2V機能の技術的ギャップ（セクション5）を明らかにし、この分野の核心的課題である「一貫性」（セクション6）と、それを解決するための最新の学術的研究（セクション7）を詳述する。最後に、これらの分析に基づき、ユースケース別の戦略的なソリューション選定ガイド（セクション8）を提示する。

## **セクション2: ベンチマーク分析: Nvidia CosmosとフィジカルAI**

ユーザーがベンチマークとして提示したNvidia Cosmosの分析は、本調査の出発点として不可欠である。しかし、Cosmosは一般的なクリエイティブ動画生成ツールとは根本的に異なる目的とアーキテクチャを持つ、高度に専門化されたプラットフォームである。

### **2.1 Cosmosの定義: 「フィジカルAI」特化プラットフォーム**

Nvidia Cosmosは、映画やソーシャルメディア向けの動画を生成するSaaS（Software as a Service）ではない。これは、「フィジカルAI」の開発者、すなわち自動運転車（AVs）、ロボット工学、および高度なビデオ分析エージェントを構築するエンジニアや研究者向けに設計された、包括的なプラットフォームである 1。

Cosmosの主たる目的は、一般的な視覚的魅力ではなく、**物理法則に準拠した (physics-aware)** 世界の状態（world states）と動画（シミュレーション）を生成することにある 2。NVIDIAはこれを実現するために「World Foundation Models (WFMs)」と呼ばれる基盤モデル群を提供しており、これらはロボットやAVが現実世界で遭遇しうる膨大なシナリオ（特に、現実では稀だが危険な「エッジケース」）の訓練データを、合成的に（synthetically）生成するために使用される 1。

この産業的かつシミュレーション中心の焦点は、NVIDIAが発表している提携先からも明らかである。Uberは、NVIDIAのDRIVE AGXプラットフォームとCosmos AIインフラストラクチャを使用して、レベル4のロボタクシー・ネットワークの安全なスケーリングを加速している 1。AVツールチェーン・プロバイダーであるForetellixや、自動運転AIモデルを開発するWayveも、Cosmosを訓練シナリオの生成やエッジケースの検索・評価に活用している 8。

### **2.2 World Foundation Models (WFMs) の機能別コンポーネント**

Cosmosプラットフォームは、単一のモデルではなく、シミュレーションデータの「生成 \-\> 拡張 \-\> 評価」という閉ループ・パイプライン（Closed-Loop Pipeline）を形成する、機能的に分化した複数の基盤モデル群によって構成されている 1。

* Cosmos Predict (2.5):  
  これが、最も一般的に「動画生成」と認識される機能を持つコンポーネントである。Cosmos Predictは、マルチモーダル入力（テキスト、画像、動画プロンプト）に基づき、動的な環境における将来の「世界の状態」を予測・生成する 12。GTC（GPU Technology Conference）や研究発表によれば、最新のCosmos Predict 2.5は、単一の入力フレームから最大30秒という、クリエイティブAIモデル（通常8〜12秒）よりも大幅に長い連続した動画シミュレーションを生成する能力を持つ 1。さらに、AVやロボティクスの複雑な3D認識に必要な、マルチビュー出力（例：異なるカメラアングルからの同時シミュレーション）をサポートしている 12。  
* Cosmos Transfer (2.5):  
  このコンポーネントは、データの「増幅（Amplification）」と「拡張（Augmentation）」に特化している。Cosmos Transferは、単一のシミュレーション（例：晴天の日中の走行データ）や、CARLAやNVIDIA Isaac Simといった物理シミュレーション・フレームワークからの3D入力を受け取り、それを多様な環境条件（例：雨天、夜間、異なる照明）へと迅速にスケーリング（転移）させる 1。これにより、開発者は、コストのかかる現実世界のデータ収集や手動でのシミュレーション設定変更を行うことなく、訓練データセットの多様性を指数関数的に増大させることが可能になる 1。  
* Cosmos Reason:  
  このモデルは、パイプラインの「批評家（Critic）」または「評価者」として機能する。Cosmos Reasonは、PredictとTransferによって生成された膨大な合成ビジュアル（動画）を評価し、その内容が妥当か、プロンプトに準拠しているかを判断する 10。Chain-of-Thought (CoT) 推論を用いてシミュレーション結果を評価し、有用なデータのみを選別したり、データキュレーション（整理・分類）を高速化するためのキャプションを自動生成したりする 1。

この「Predict（生成）-\> Transfer（拡張）-\> Reason（評価）」という三位一体のアーキテクチャは、Cosmosがクリエイティブな表現の追求ではなく、スケーラブルで高品質な「AI訓練用データファクトリー」として設計されていることを明確に示している。

### **2.3 入力モダリティ: I+T2Vの具体的な使用法**

Cosmos WFM群は、テキスト、画像、動画プロンプトを含むマルチモーダル入力をサポートしている 12。Hugging Faceで公開されているオープンモデル nvidia/Cosmos-1.0-Diffusion-14B-Text2World は、その名の通りテキスト入力（最大300語のシーン記述）から5秒の動画（1280x704）を生成する仕様が確認できる 2。

ユーザーのクエリである「画像とテキストの入力」に最も近いのは、同じくCosmos 1.0リリースに含まれる Cosmos-1.0-Diffusion-14B-Video2World モデルである 2。このモデルのアーキテクチャ説明によれば、テキスト記述（プロンプト）と\*\*「画像（最初のフレームとして）」\*\*を受け取り、未来の120フレーム（24fpsで5秒間）を予測する 2。

ここでのI+T2Vの使われ方は、クリエイティブAIのそれとは異なる。クリエイティブAIが「この人物（画像）が、踊っている（テキスト）シーン」を生成するのに対し、CosmosのI+T2Vは「この交差点の状況（画像）で、前方の車が急ブレーキを踏んだ（テキスト）場合の未来（動画）」をシミュレートするために使用される。入力画像は「アートの参照」ではなく、「シミュレーションの初期状態」として機能する。

### **2.4 ライセンスと商用利用: NVIDIA Open Model License**

Nvidiaは、Cosmosモデル群を「NVIDIA Open Model License Agreement」（バージョンリリース日: 2025年1月6日）の下で公開している 2。

このライセンスは、NVIDIAのモデルが**商用利用可能**であることを明記している 2。ライセンス契約に基づき、ユーザーは派生モデル（Derivative Models）を自由に作成・配布でき、NVIDIAはモデルや派生モデルを使用して生成された「出力（Outputs）」の所有権を主張しない 2。これは、Cosmosを使用して生成したシミュレーション・データを、自社のAIモデルの訓練に（商用目的で）自由に使用できることを意味する。

ただし、このライセンスには極めて重要な制限事項と義務が課されている。

1. **安全ガードレールの尊重:** ライセンスは、ユーザーがモデルに含まれる技術的な制限、\*\*「安全ガードレール (safety guardrail)」\*\*または関連する安全ハイパーパラメータを、意図的にバイパス、無効化、または回避した場合、自動的に終了する 2。これは、NVIDIAがフィジカルAIの「安全性」という側面において厳格な制御を保持し、モデルの不正利用や危険な利用を防ぐための法的枠組みを維持していることを示している。  
2. **表示義務:** Cosmosモデル、またはそれを含む製品やサービスを配布・利用する場合、あるいはAIモデルの訓練・改善に使用する場合は、関連するウェブサイト、UI、または製品ドキュメントに **"Built on NVIDIA Cosmos"** と明確に表示することが義務付けられている 2。

このライセンス体系は、NVIDIAがオープンなエコシステムを推進しつつも、特に安全性がクリティカルとなる「フィジカルAI」ドメインにおいて、NVIDIAプラットフォームへの依存とコントロールを維持するという高度な戦略を反映している。

## **セクション3: 主要クラウドAPIプラットフォームの徹底比較 (クリエイティブ・ドメイン)**

Nvidia Cosmosが「フィジカルAI」という専門ドメインを確立している一方、ユーザーが本来関心を持っているであろう「クリエイティブAI」ドメインでは、GoogleとOpenAIがAPI（Platform as a Service）レイヤーで激しい覇権争いを繰り広げている。このセクションでは、Google VeoとOpenAI SoraのAPI仕様、I+T2V機能、および価格体系を詳細に比較分析する。

### **3.1 Google Veo (Vertex AI / Gemini API)**

Google Veoは、Google I/Oなどで発表された、同社の最先端動画生成モデルである。リアリズム、プロンプトへの忠実性、そして特筆すべき**ネイティブオーディオ生成能力**において、市場の最高水準を定義している 4。

* APIアクセス:  
  Veoは、Google Cloudのエンタープライズ向けAIプラットフォームである Vertex AI 16、および開発者向けの Gemini API 14 を通じて、プレビュー版または正式版として提供されている。  
* I+T2V機能 (API仕様):  
  VeoのAPIは、I+T2Vに関して非常にリッチで多機能な制御インターフェースを提供するように設計されている。  
  1. **標準のI+T2V:** Vertex AI APIは、instances オブジェクト内で prompt (テキスト) と image (base64エンコードされた画像データまたはGCS URI) を受け取る 16。入力画像は image/jpeg または image/png 形式がサポートされる 16。推奨される入力解像度は720p以上である 16。  
  2. **リファレンス画像 (Veo 3.1):** Veo 3.1のAPIは、referenceImages パラメータをサポートする 16。これにより、**最大3枚の参照画像**（アセット画像またはスタイル画像）を入力として提供し、生成される動画全体のキャラクターやスタイルの一貫性を強力にガイドすることができる 17。  
  3. **高度な制御 (Veo 3.1):** Veoは、他のモデルには見られない高度な編集機能を提供する。  
     * **動画延長 (Video extension):** Veoで生成された既存の動画クリップを入力し、その続きをシームレスに生成（延長）する機能 17。  
     * **最初と最後のフレーム指定:** APIリクエストで image (第1フレーム) と lastFrame (最終フレーム) の両方を指定できる 16。これにより、モデルはその2つの静止画像間を補間する、滑らかなトランジション動画（例：開始時の歌手の正面ショットから、終了時のステージ後方からのPOVショットへの180度のアークショット）を生成できる 18。  
* **技術仕様 (出力):**  
  * **モデル:** Veo 3.1 および Veo 3.1 Fast (より高速で低コストなバージョン) 14。  
  * **長さ:** Gemini API経由のVeo 3.1は**最大8秒** 17。Vertex AI経由のVeo 3.0は 4秒、6秒、または 8秒 のオプションが提供される 19。  
  * **解像度:** **720p** または **1080p** の出力をサポート 17。  
  * **オーディオ:** Veoの最大の差別化要因の一つ。BGMや効果音（SFX）だけでなく、プロンプト内で引用符（例: A woman says, "We have to leave now."） 18 を使用して指定された**セリフ（Dialogue）を生成し、動画内のキャラクターの口の動きとほぼ完璧にリップシンク**させるネイティブオーディオ生成機能を備えている 4。  
* 価格 (2025年時点):  
  Veoの価格設定は、その高度な機能を反映し、非常に高価なエンタープライズ向けとなっている。  
  * **Veo 3 (API/Vertex AI):** 生成された動画（オーディオ含む）1秒あたり **$0.75** 20。  
  * *コスト分析:* この価格設定では、標準的な8秒のクリップ（1080p）を1本生成するのに $6.00 かかる 20。これは1分あたり $45、1時間あたり $2,700 に相当する。Veo 2の旧価格（$0.50/秒） 22 からもさらに値上げされており、GoogleがVeoを（少なくとも現時点では）大衆向けツールではなく、高マージンのB2B/エンタープライズ・ソリューションとして位置づけていることが明確に示されている。

### **3.2 OpenAI Sora (Azure AI / OpenAI API)**

OpenAI Soraは、その発表時に業界に衝撃を与えたフラッグシップ動画生成モデルであり、複雑なシーン、長時間のコヒーレンス、および物理的な世界の動きのシミュレーションにおいて驚異的な能力を示す 6。

* APIアクセス:  
  Soraは、Microsoftの Azure AI Service (Azure OpenAI Foundryの一部として) 25 および OpenAI API 26 を通じて、選定されたパートナーや開発者向けにプレビュー提供されている。  
* I+T2V機能 (API仕様):  
  SoraのAPIは、Veoと比較すると、よりシンプルで直感的なI+T2V機能を提供する。  
  1. **リファレンス画像:** Sora 2 APIは input\_reference というパラメータをサポートする 25。  
  2. **使用法:** この input\_reference は、\*\*「動画の最初のフレームとして機能するビジュアルアンカー」\*\*として定義されている 25。開発者は、cURLやPythonリクエストにおいて、prompt (テキスト) と共に、ローカルの画像ファイル（例: sample\_720p.jpeg）を input\_reference として指定する 26。これにより、入力画像（例：特定の人物やオブジェクト）を起点として、テキストプロンプト（例："She turns around and smiles, then slowly walks out of the frame." 26）が指示する動きやアクションを実行するI+T2Vが実現される。  
  3. **制限:** Azureのドキュメント 25 によれば、input\_reference として提供される画像の解像度は、size パラメータで指定される出力動画の解像度（例: 1280x720）と**完全に一致**する必要がある。これは、Veoの柔軟なリファレンス機能と比較すると、より厳格な制約である。  
* **技術仕様 (出力):**  
  * **モデル:** Sora 2 (標準モデル) および Sora 2 Pro (高品質・高解像度モデル) 25。  
  * **長さ:** Azure APIでは **4秒、8秒、12秒** のオプションが提供される 25。最大12秒という長さは、Veoの最大8秒を上回る利点である。  
  * **解像度:** 720x1280 (Portrait) または 1280x720 (Landscape) が標準。Sora 2 Proは、より高解像度な 1024x1792 / 1792x1024 もサポートする 25。  
  * **オーディオ:** Soraもネイティブの音楽、効果音、ダイアログ生成をサポートしている 6。  
* 価格 (2025年時点):  
  OpenAIは、GPT-3/4の成功体験に基づき、市場シェアの獲得を優先する、非常に戦略的な多層価格設定をSoraに導入している。  
  * **Sora 2 (Standard):** 720p解像度で、1秒あたり **$0.10** 27。  
  * **Sora 2 Pro (720p):** 720p解像度（高品質）で、1秒あたり **$0.30** 27。  
  * **Sora 2 Pro (High-Res):** 1080p以上の高解像度で、1秒あたり **$0.50** 27。  
  * *コスト分析:* Veo 3の$0.75/秒 \[20\] と比較すると、Sora 2 Pro (720p) の$0.30/秒は60%も安価であり、Sora 2 (Standard) の$0.10/秒に至っては86%も安価である。この価格差は、APIを組み込む開発者やスタートアップにとって決定的な要因となり得る。さらに、Kie.ai \[28\] のようなサードパーティのAPIリセラー（再販業者）は、Sora 2 Pro (720p) を公式価格の$0.30/秒を大幅に下回る$0.045/秒といった価格で提供しており、OpenAIがエコシステムの拡大を（少なくとも現時点では）優先していることを示唆している。

### **3.3 表1: 主要クラウドAPI (Veo vs. Sora) 機能・価格比較表**

| 機能 | Google Veo 3.1 (via Vertex/Gemini API) | OpenAI Sora 2 (via Azure/OpenAI API) |
| :---- | :---- | :---- |
| **I+T2V入力** | **対応 (高機能)** | **対応 (標準)** |
| APIパラメータ | image (I2V), referenceImages (最大3枚, スタイル/キャラ), lastFrame (トランジション) 16 | input\_reference (1枚, 第1フレームのアンカー) 25 |
| **最大動画長** | 8秒 17 | **12秒** 25 |
| **解像度** | 720p / 1080p 17 | 720p / 1080p (Sora 2 Pro) 27 |
| **オーディオ生成** | **対応 (ネイティブ、SFX、リップシンク対応)** \[14, 18\] | 対応 (ネイティブ、SFX、ダイアログ) 6 |
| **価格 (Standard)** | $0.75 / 秒 (Veo 3, 720p/1080p) 20 | **$0.10 / 秒** (Sora-2, 720p) 27 |
| **価格 (Pro)** | N/A (Veo 3は単一ティア) | **$0.30 / 秒** (Sora-2-Pro, 720p) 27 |
| **価格 (High-Res)** | $0.75 / 秒 20 | **$0.50 / 秒** (Sora-2-Pro, 1080p+) 27 |

この比較から導かれる戦略的結論は明白である。Google Veoは、高価（$0.75/s）だが、マルチリファレンス、フレーム指定、リップシンクといった、既存の映像制作ワークフローに深く統合可能な**高機能・高マージンなエンタープライズ製品**として位置づけられている。対照的に、OpenAI Soraは、圧倒的な低価格（$0.10/s〜）とより長い動画長（12秒）を武器に、開発者やスタートアップ（リセラー含む）による**エコシステムの急速な構築と市場シェアの最大化**を狙う、プラットフォーム戦略を採用している。

## **セクション4: 商用SaaSソリューションの機能比較**

SoraやVeoのようなAPIレイヤー（PaaS）の下には、これらのAPIをラップするか、あるいは独自の基盤モデルをホストし、クリエイターや小規模チームに対して月額課金（サブスクリプション）で機能を提供するSaaS（Software as a Service）レイヤーが存在する。このレイヤーは、APIの難解な秒単位課金のリスクを吸収し、予測可能なコストで直感的なUI（ユーザーインターフェース）を提供することに価値がある。

### **4.1 Adobe Firefly (Adobe Creative Cloud)**

Adobeは、同社の強力なCreative Cloudエコシステム（Photoshop, Premiere Pro, After Effects）に、Fireflyブランドの生成AI機能を深く統合することで、競合他社に対する決定的な優位性を築こうとしている。

* **機能:** 「Image to video」機能がFireflyに搭載されている 7。これは、静的なスケッチ、画像、写真を動画に変換する機能である 7。  
* **I+T2V制御:** Adobeのアプローチは、プログラマティックなテキストプロンプトよりも、非技術者であるデザイナーや編集者に最適化された**UIベースの制御**に重点を置いている。  
  1. **画像入力:** ユーザーは静止画像（写真、イラスト、AI生成画像）をアップロードする 7。  
  2. **テキスト入力:** テキストプロンプトは、動きの精密な指示よりも、動画が喚起すべき「フィーリング（雰囲気）」を指定するために使用される 7。  
  3. **UI制御:** 実際の「動き」の制御は、UI上のシンプルなコントロール（例：ズーム、パン、チルト、方向性のある動き）や、ショットタイプの選択（例：ワイドショット、クローズアップ）によって行われる 7。  
* **特徴:** 生成される動画は、Adobeがライセンス許諾したコンテンツとパブリックドメインのデータのみで訓練されたFireflyモデルに基づいているため、**商用利用に対して安全 (Commercially Safe)** であることが保証されている点である 7。出力はPremiereやAfter Effectsに直接エクスポート可能である 7。  
* **価格:** クレジットベースのサブスクリプション 29。  
  * **Firefly Standard:** $9.99/月。2,000クレジットが含まれ、5秒の動画を最大20本生成可能 30。  
  * **Firefly Premium:** $199.99/月。50,000クレジットに加え、Firefly Videoモデルへの**無制限アクセス**が含まれる（2024年12月1日までのプロモーション情報も存在するが、標準プランではPremiumが無制限アクセスを提供） 30。

### **4.2 Runway (Gen-2, Gen-3 Alpha, Gen-4)**

Runwayは、AI動画編集のパイオニアであり、Text-to-Videoだけでなく、Image-to-Video (I2V), Video-to-Video (V2V) など、総合的なAI動画編集ツールキットを提供している。

* **機能:** Runwayは、Gen-2, Gen-3, Gen-4といったモデルの進化と共に、「Image \+ Text-to-Video」モードを明示的にサポートしている 33。  
* **I+T2V制御:** Runwayは、AdobeのUIベースのアプローチと、Pikaのパラメータベースのアプローチの中間に位置する、**意味論的なテキストプロンプトによる制御**を提供する。  
  1. **画像入力:** リファレンス画像を入力する 33。  
  2. **テキスト入力:** テキストプロンプトを使用して、その画像に適用する「スタイル」（例：'masterpiece', 'cinematic'）、「アクション」（例：'flying', 'running'）、および「カメラワーク」（例：'pan', 'zoom'）を具体的に指示する 33。  
  3. **高度なワークフロー:** Runwayのコミュニティでは、静的なリファレンス画像（キャラクター）と、別途撮影したビデオ（動きのソース）を組み合わせ、V2Vのスタイル変換を利用して、リファレンス画像のスタイルを維持したまま動きを生成する、より高度な（Gen-1時代からの）ワークフローも議論されている 35。  
* **価格:** クレジットベースのサブスクリプション 36。  
  * **Standard Plan:** $12/月 (年払)。625クレジット 37。  
  * **Pro Plan:** $28/月 (年払)。2250クレジット 37。このクレジットで、Gen-4 Turbo (最速モデル) を450秒、またはGen-4 (高品質モデル) を187秒生成できる 36。

### **4.3 Pika Labs (Pika 1.0 \- 2.5)**

Pika (旧Pika Labs) は、当初Discordボットとして登場し、その強力なI+T2V機能で急速に普及したSaaSプラットフォームである。

* **機能:** Text-to-Video, Image-to-Video, Video-to-Videoをサポート 38。  
* **I+T2V制御:** Pikaは、Runwayよりもさらに技術的で**精密なパラメータ制御**をテキストプロンプト内で直接行う点に特徴がある。  
  1. **画像入力:** Discordでは /animate コマンドで画像をアップロードする 40。  
  2. **テキスト入力:** オプションのテキストプロンプトを追加できる 40。  
  3. **パラメータ制御:** Pikaの真価は、プロンプトの末尾に付加する制御パラメータにある 40。  
     * \-camera \[zoom in/out, pan up/down/left/right, rotate clockwise\]：カメラワークを明示的に指定。  
     * \-motion \[0-4\]：動きの強さを0（静止）から4（激しい）まで数値で制御。  
     * \-ar \[ratio\], \-fps \[8-24\], \-gs \[8-24\]：アスペクト比、フレームレート、ガイダンススケールも直接制御可能 40。  
* **価格:** クレジットベースのサブスクリプション。  
  * **Free Plan:** 80クレジット/月 41。  
  * **Standard Plan:** $8/月。700クレジット 41。ウォーターマークなし、商用利用可 41。  
  * **Pro Plan:** $28/月。2300クレジット 42。  
* *分析:* Pikaは、RunwayのProプランとほぼ同額（$28/月）で同等（2300）のクレジットを提供しつつ、より安価なStandardプラン（$8/月）も用意しており、高い価格競争力と、パワーユーザー向けの精密な制御機能（パラメータ）を両立させている。

### **4.4 表2: 商用SaaSプラットフォーム (Adobe vs. Runway vs. Pika) 比較表**

| プラットフォーム | I+T2Vの主な機能 | 制御方法 | 価格モデル (Pro Plan 目安) | エコシステムと特徴 |
| :---- | :---- | :---- | :---- | :---- |
| **Adobe Firefly** | Image to video 7 | **UIによるカメラ制御** (Pan, Zoom) \+ テキストによる雰囲気指定 7 | $19.99/月 (Firefly Pro, 4,000 C) または $199.99/月 (Premium, 無制限Video) \[30\] | **Creative Cloud (Premiere, Photoshop) とのシームレスな統合。** 商用セーフ (Commercially Safe) 7。 |
| **Runway** | Image \+ Text-to-Video (Gen-2/4) 33 | **テキストプロンプトによる意味論的制御** (スタイル、アクション、カメラワーク) 33 | $28/月 (Pro Plan, 2250 C) 37。 (Gen-4 Turboで450秒分 36) | 総合的なAI動画編集（V2V, Inpaintingなど）機能が豊富。プロクリエイター向け 43。 |
| **Pika Labs** | /animate \+ テキストプロンプト 40 | テキストプロンプト \+ **詳細なパラメータ制御** ( \-camera, \-motion ) 40 | $28/月 (Pro Plan, 2300 C) 42。 (Standard $8/月〜) | 高度なパラメータ制御によるI+T2Vに特化。価格競争力が高い 42。 |

SaaSレイヤーの分析は、APIレイヤー（セクション3）の分析と組み合わせることで、重要な示唆を導き出す。例えば、Sora 2 Pro APIのコストは$0.30/秒である 27。RunwayのProプラン（$28/月）で利用可能な450秒 36 を、Sora APIで直接生成した場合、コストは $0.30 \\times 450 \= $135 となる。

この$135（APIコスト）と$28（SaaS会費）という価格差こそが、SaaSレイヤーの存在意義である。SaaS企業（Runway, Pika）は、Soraや自社モデルのAPI（秒単位課金）という高価で高リスクなインフラコストを自社で引き受け、それをクリエイターにとって予測可能で安価な「月額固定料金（サブスクリプション）」に変換するという「金融商品（リスクアグリゲーター）」として機能している。

したがって、開発者は「自社でAPIを直接叩き、高コストで試行錯誤のリスクを負う」か、「SaaSの安価なサブスクリプションを利用し、低リスクで迅速なイテレーション（試行錯誤）を行う」か、という戦略的選択を迫られることになる。APIの従量課金はイテレーションを「罰する」が、SaaSのサブスクリプションはそれを「奨励する」のである。

## **セクション5: オープンソースモデルの現状と「I+T2V」の壁**

クローズドなAPIやSaaSプラットフォームに対する選択肢として、オープンソースモデル（またはウェイト公開モデル）の動向分析は不可欠である。この分野で最も広く普及しているのが、Stability AIによるStable Video Diffusion (SVD) である。

### **5.1 Stability AI: Stable Video Diffusion (SVD)**

Stable Video Diffusionは、絶大な人気を誇る画像生成モデルStable Diffusionをベースに、動画生成タスク（Image-to-Video）にファインチューンされたモデルである 44。

* **モデル:** SVDは複数のバージョンがリリースされている。初期の img2vid は14フレーム、img2vid-xt は25フレームを576x1024の解像度で生成する 44。その後、SVD 1.1としてさらに高品質なファインチューンが施されたモデルもリリースされている 44。  
* **入力仕様:** モデル名が stable-video-diffusion-img2vid-xt 45 となっていることからも明らかなように、SVDのコア機能は **Image-to-Video (I2V)** である。これは、静止画像を「コンディショニング・フレーム」として入力し、そこから短い動画クリップ（14〜25フレーム）を生成する 45。  
* I+T2Vの欠如 (明確な限界):  
  ユーザーのクエリである「画像とテキスト」による制御（I+T2V）に関して、SVDの仕様は明確である。Hugging Faceで公開されている公式のモデルカード 45、およびCivitaiのクイックスタートガイド 44 の両方で、制限事項として「The model cannot be controlled through text.（このモデルはテキストを通じて制御できない）」と明確に記載されている。  
  SVDの「動き」の制御は、テキストプロンプト（例：「歩き出す」）によって行われるのではない。その代わりに、以下のような数値パラメータによって制御される 44。  
  * **motion\_bucket\_id**: 動きの「量」や「激しさ」を指定する数値（値が大きいほど動きが激しくなる）。  
  * **fps**: 生成される動画のフレームレート。  
  * **augmentation level**: 入力画像に追加するノイズの量。この値を大きくすると、入力画像への忠実度（一貫性）は下がるが、より大きな動き（モーション）が生成される傾向がある 44。

つまり、SVDは「画像（入力）」から「動き（パラメータ）」を生成するI2Vモデルであり、SoraやVeoのような「画像（参照）」と「テキスト（指示）」を組み合わせて意味論的な動画を生成するI+T2Vモデルではない。

* ライセンス:  
  SVD 1.0および1.1のウェイトは、非商用 (Non-Commercial) ライセンス（例：Stability AI's Non-Commercial Research Community License）の下でリリースされている 45。商用利用を行うには、Stability AIとの別途のライセンス契約が必要となる 45。これは、Nvidia Cosmosのオープンモデル 2 がデフォルトで商用利用を許可している点と対照的である。

### **5.2 Hugging Face エコシステム**

Hugging Faceのモデルハブには、SVD以外にも多数のImage-to-Videoモデルが登録されている（"image-to-video" パイプラインタグで425件以上がヒットする） 46。

Wan-AI/Wan2.2-I2V-A14B 46 や I2VGen-XL 49 といったモデルがトレンドの上位にあり、これらはSVDと同様に、静止画像から高品質なアニメーションを生成する能力において高い評価を得ている。

しかし、これらのモデルの多くも、SVDと同様にI2Vに特化しているか、あるいはT2V（Text-to-Video） 51 に特化しているかのどちらかである。2025年現在、SoraやVeoが実現しているような、トランスフォーマーアーキテクチャをベースに、「画像リファレンス」と「テキストプロンプト」という異なるモダリティの入力を高度に融合させ、意味論的な編集（例：画像内の人物に、テキストで指示した複雑な行動を取らせる）を可能にするオープンソースモデルは、まだ主流となっていない。

この分析から明らかになるのは、現在のオープンソース（SVD）とクローズドAPI（Sora/Veo）の間には、単なる動画の品質や長さといった「性能差」以上に、**「I+T2Vのサポート」というアーキテクチャ的な断絶**が存在するということである。

SVDは、画像モデル（Stable Diffusion）のアーキテクチャを「ファインチューン」することでI2V機能を実現した。そのため、テキストプロンプトを意味論的な「動きの指示」として解釈する能力を、アーキテクチャの根本レベルで持っていない 45。  
一方で、SoraやVeoは、テキストと画像を同等に扱う「マルチモーダル・トランスフォーマー」（具体的にはDiffusion Transformer (DiT) 52）として、ゼロから設計されている。これらのトランスフォーマーアーキテクチャは、テキスト埋め込みと画像埋め込みを、アテンション機構を通じてシームレスに統合し、一方（画像）を参照しながらもう一方（テキスト）で制御することを可能にする。  
結論として、「I+T2V」という高度な制御機能は、オープンソースコミュニティに対する、SoraやVeoといったAPIプロバイダーの**決定的な「堀」 (moat)** として機能している。オープンソースが（テキスト制御できない）I2Vに留まっている限り、ユーザーは被写体の一貫性や複雑なシーン制御といった高度な要求を満たすために、高価な商用API/SaaSにロックインされ続ける構造となっている。オープンソースコミュニティがこの差を埋めるには、SVDの改良ではなく、Sora/Veoクラスの大規模なマルチモーダル・トランスフォーマーをゼロから訓練するという、膨大な計算リソースを必要とする挑戦が不可欠となる。

## **セクション6: 中核的技術課題：時間的・空間的一貫性 (Spatiotemporal Consistency)**

なぜI+T2Vが技術的に困難であり、SVDのようなオープンソースモデルとSoraのようなクローズドAPIとの間に明確な壁が存在するのか。その中核には、「時間的・空間的一貫性（Spatiotemporal Consistency）」という、動画生成AIに固有の、極めて困難な技術的課題が存在する。

### **6.1 課題の定義: 「アイデンティティ・ドリフト」**

画像生成AI（例：Stable Diffusion）は、1枚の静止画像を生成すればタスクは完了する。しかし、動画生成AIは、1秒間に24フレーム（24枚の画像）を、例えば8秒間（合計192フレーム）にわたって生成し続けなければならない。

このプロセスにおける最大の課題が、生成されるフレーム間（時間軸）で、被写体や背景の**一貫性**を維持することである 9。特に、参照画像（Input Image）で指定された被写体（例：特定の人物の顔、特定のロゴが入ったTシャツ）が、動画の再生中に微妙に、あるいは時には劇的に、**別物**に変化してしまう現象が頻発する。

これは「**アイデンティティ・ドリフト (Identity Drift)**」または「時間的コヒーレンスの欠如」と呼ばれる 53。ほんの数秒のクリップであっても、フレームが進むにつれて顔の造形が崩れたり、服装のディテールが「モーフ（変異）」したり、背景のオブジェクトが不自然に出現・消失したりする 56。

この問題の根本的な原因は、多くの拡散モデル（Diffusion Models）が、各フレーム（またはフレームのパッチ）を（部分的に）独立してサンプリングするプロセスに起因する 55。各フレームの生成時に生じる潜在空間（Latent Space）内でのわずかな「ズレ（Drift）」が、時間軸に沿って蓄積・増幅される結果、数秒後には入力画像とは似ても似つかぬ「別人」になってしまうのである 55。

この「アイデンティティ・ドリフト」こそが、現在のAI動画生成が「面白いオモチャ」から「プロフェッショナルな制作ツール」へ移行する上で、最大の障害となっている 54。

### **6.2 I+T2Vにおける課題の増幅**

この一貫性の問題は、ユーザーの要求である「Image-and-Text-to-Video (I+T2V)」において、さらに深刻化する。なぜなら、I+T2Vはモデルに対して、本質的に**相反する2つの要求**を同時に突きつけるからである。

1. **維持 (Consistency):** 入力画像（Reference Image）のアイデンティティ（顔、服装、スタイル）を、動画の全フレームにわたって**忠実に維持**せよ。  
2. **変化 (Motion):** 入力テキスト（Text Prompt）の指示（例：「振り向きながら笑う」「走り出す」）に従い、入力画像の状態から**大胆に変化**せよ。

モデルは、「参照画像への忠実性」と「テキストプロンプトによる変化」という2つの力の間の、困難なトレードオフを管理しなければならない。テキストが複雑な動き（例：激しいダンス、顔の表情の変化）を指示すればするほど、モデルはプロンプトに従おうとして入力画像からの「逸脱」を余儀なくされ、その結果「アイデンティティ・ドリフト」が加速する 54。

逆に、一貫性を過度に重視するモデル（例：motion\_bucket\_idを低く設定したSVD 44）は、入力画像が微かに揺れるだけ（"Cinemagraph"）の、テキスト指示を無視した退屈な動画しか生成できなくなる。

SoraやVeoがSVDよりも優れていると評価される根本的な理由は、彼らの巨大なトランスフォーマーアーキテクチャが、この「維持 vs 変化」のトレードオフを、従来のモデルよりもはるかに高度なレベルで（完璧ではないにせよ）管理できる点にある。彼らは、入力画像を単なる「最初のフレーム」としてだけでなく、テキストプロンプトの「強力なアンカー」として活用し、ドリフトを抑制しながら指示された動きを実行できるのである 57。

### **6.3 サーベイ論文に見る技術的アプローチ**

arXivで公開されたサーベイ論文 "A Survey: Spatiotemporal Consistency in Video Generation" (2025年2月) 9 は、この「時間的・空間的一貫性」の問題を解決するための最新の技術的アプローチを体系的に分類している。

この論文によれば、一貫性を維持するための研究は、以下の5つの主要な側面に分類される 9：

1. **基盤モデル (Foundation Models):** GAN、拡散モデル、トランスフォーマーなど、一貫性を維持する上で数学的に有利なモデルアーキテクチャ自体の探求 9。  
2. **情報表現 (Information Representations):** 時空間パッチング（Spatio-temporal Patching、Soraが採用 53）やVAE、自己注意機構（Self-Attention）を用い、フレーム間の時間的な依存関係を効率的に表現・学習する手法 60。  
3. **生成スキーム (Generation Schemes):** マスクモデル（Masked Prediction）などを用い、時間的シーケンスの文脈（前後のフレーム）を理解させながら生成する手法 60。  
4. **後処理 (Post-processing Techniques):** 一度生成された動画に対し、時間軸または空間軸で最適化（例：フレーム間の平滑化）を加え、品質を向上させる手法 9。  
5. **評価指標 (Evaluation Metrics):** 生成された動画の一貫性を、人間（Human Evaluation） 49 やAI（例：FVD, CLIP Score 61）で正確に測定し、モデル改善のフィードバックループを回すための指標。

真の課題はもはや「動画を生成すること」ではなく、「**参照（Identity）を維持したまま、指示（Motion）を実行すること**」である。この二つの競合する要求をいかに両立させるかが、次世代モデルの主戦場となっている。

## **セクション7: 最先端研究：一貫性問題のアーキテクチャ的解決策**

セクション6で定義した「維持（Identity） vs 変化（Motion）」という中核的トレードオフを解決するため、2024年後半から2025年にかけて、この問題に正面から取り組む重要な学術研究が登場している。これらの研究は、SoraやVeoの「次」のアーキテクチャを予見させるものであり、特に「**分離（Disentanglement）**」という設計思想において共通点が見られる。

### **7.1 アプローチ1: ConsisID (周波数分解によるアイデンティティ維持)**

* **論文:** "Identity-Preserving Text-to-Video Generation by Frequency Decomposition" (CVPR 2025\) 62。  
* **課題:** 既存のIPT2V (Identity-Preserving Text-to-Video) モデルは、テキストによる編集可能性（Motion）を追求すると顔の一貫性（Identity）が崩壊し、一貫性を追求すると編集可能性が失われるというジレンマを抱えている 62。  
* 中核的アイデア（周波数分解）:  
  ConsisIDは、顔のアイデンティティ情報を「周波数」ドメインで分離するという、独創的なアプローチを採用する 63。  
  1. **低周波 (Low-frequency):** 顔の輪郭、プロファイル、比率といった、ポーズや表情によって変化する**グローバルな特徴**。  
  2. **高周波 (High-frequency):** 目や鼻の具体的な形状、肌のテクスチャなど、ポーズが変わっても変化しない**固有のアイデンティティ・マーカー**。  
* 技術的実装:  
  この「分離」されたID情報を、Diffusion Transformer (DiT) ベースの動画生成モデルの\*\*適切な場所（レイヤー）\*\*に注入する 64。DiTの分析から、低周波情報はモデルの「浅いレイヤー」に、高周波情報はアテンションブロック内の「ビジョントークン」に注入するのが最適であることを突き止めた 64。  
  具体的には、顔認識バックボーンから抽出した浅い（低周波）特徴をCLIP特徴量と連結し、深い（高周波）特徴をクロスアテンション機構を介してDiTの各アテンションブロックに注入する 65。  
* **結果:** この周波数アウェアな制御スキームにより、ConsisIDは、テキストプロンプトによる高い編集可能性（Motion）を維持しつつ、参照画像のアイデンティティ（Identity）を既存のSOTA（State-of-the-Art）モデルよりも高い忠実度で維持することに成功している 62。

### **7.2 アプローチ2: S2V (IDとモーションの分離学習)**

* **論文:** "Subject-driven Video Generation via Disentangled Identity and Motion" (arXiv:2504.17816) 61。  
* **課題:** 被写体主導（Subject-driven）の動画生成（S2V）を行う上で、最大の障壁は「学習データ」である。特定の被写体（例：あなたの飼い猫）が、テキストで指示された任意の動き（例：月面を歩く）をしている「被写体と動きのペア学習データ」は、この世に存在しない 61。  
* 中核的アイデア（タスク分離）:  
  この研究は、S2Vの学習タスクそのものを、「アイデンティティ学習」と「モーション学習」という2つの独立したタスクに\*\*分離（Disentangled）\*\*する。  
  1. **ID注入 (ID Injection):** 被写体のアイデンティティを学習するタスク。これには、被写体の**画像データセット**（動画は不要）のみを使用する 61。  
  2. **時間認識の保存 (Temporal-aware Preservation):** 動画の自然な動き（モーション）を学習するタスク。これには、被写体とは**無関係**の、一般的な「テキストと動画のペア」データセット（例：ウェブから収集した動画）を使用する 67。  
* 技術的実装:  
  ファインチューニングの過程で、これら2つの異なる学習タスク（ID注入と時間認識）を、「確率的切り替え戦略 (stochastic-switching strategy)」を用いて、バッチごとまたはステップごとに交互に最適化する 61。  
  この「交互学習」により、モデルは、一方のタスク（例：ID学習）で得た知識を、もう一方のタスク（例：モーション学習）によって「破滅的に忘却（catastrophic forgetting）」することを防ぎながら、両方の能力を獲得することができる 67。  
* **結果:** このタスク分離と確率的切り替え戦略により、このモデルは、「被写体と動きのペア学習データ」を一切使用することなく、ゼロショットで、入力された被写体（Subject）がテキストで指示された動き（Motion）を行う、一貫性のある動画を生成することが可能になった 67。

### **最先端研究が示す未来**

ConsisID（周波数の分離）とS2V（学習タスクの分離）は、アプローチこそ異なるものの、奇しくも「**分離（Disentanglement）**」という共通の設計思想に収斂している。

これは、次世代のI+T2Vモデルが、単一の巨大なE2E（End-to-End）モデル（SoraやVeoの初期形態）から、**複数の制御可能な「モジュール」の集合体**へと進化することを示唆している。

画像生成の分野では、ControlNet（姿勢、深度、線画といった制御を分離・追加可能にした）が大きなブレークスルーとなった。動画生成においても同様に、「アイデンティティ（ConsisID）」「スタイル（VeoのreferenceImages）」「モーション（S2V）」「テキスト指示（プロンプト）」「カメラワーク（AdobeのUI / Pikaのパラメータ）」といった、**互いに直交する複数の制御ベクトル**を、ユーザーが個別に入力・制御できるアーキテクチャが主流となる可能性が高い。

現在のSoraやVeoのAPI（例：Veoの image, referenceImages, lastFrame という分離された入力 17）は、この「分離された制御モジュール」という未来に向けた、第一歩に過ぎない。

## **セクション8: 戦略的分析と推奨事項**

本レポートで実施したNvidia Cosmosのベンチマーク分析、主要API（Veo, Sora）およびSaaS（Adobe, Runway, Pika）の機能・価格比較、オープンソース（SVD）の技術的限界、そして一貫性に関する最新の学術研究（ConsisID, S2V）の動向に基づき、開発者、プロダクトマネージャー、およびクリエイティブ・プロフェッショナルに向けた戦略的分析と具体的な推奨事項を以下に提示する。

### **8.1 ユースケース別・最適ソリューション選定ガイド**

AI動画生成のソリューション選定における最大の過ちは、単一の「最適な」ツールを探すことである。セクション1および2で明らかにした通り、市場は「フィジカルAI（産業シミュレーション）」と「クリエイティブAI（メディア生成）」に明確に二極化しており、ユースケースの定義こそが選定の第一歩となる。

* **推奨1: 産業シミュレーション / ロボティクス / 自動運転**  
  * **ユースケース:** 自動運転（AV）のエッジケース・シミュレーション、ロボットアームのピッキング動作の訓練データ生成、デジタルツイン内での物理現象の予測。  
  * **最適ソリューション:** **Nvidia Cosmos (WFM)**  
  * **理由:** 現状、**物理的正確性 (Physics-aware)** 2、3Dシミュレーション・フレームワーク（NVIDIA Isaac Sim等）との連携 1、および最大30秒の長時間・マルチビュー予測 12 を提供できるのはCosmosのみである。クリエイティブAI（Sora/Veo）は「映画的」ではあるが「物理的」ではないため、このユースケースには不適格である。  
* **推奨2: エンタープライズ / B2Bプラットフォーム開発（API組込）**  
  * **ユースケース:** 自社プラットフォームやサービスに、Adobe FireflyやRunwayと同等、あるいはそれ以上の最先端AI動画生成機能をAPI経由で組み込みたい。  
  * **最適ソリューション:** **OpenAI Sora API** または **Google Veo API**  
  * **選定の分岐点:**  
    * **OpenAI Sora ($0.10 \- $0.30/s)** 27 を選ぶべき場合:  
      * コストパフォーマンスと市場シェアを最優先する。  
      * より長い動画長（最大12秒） 25 が必要。  
      * APIリセラー 28 の活用も視野に、スタートアップ的な迅速な開発・普及を目指す。  
    * **Google Veo ($0.75/s)** 20 を選ぶべき場合:  
      * コストよりも高度な制御機能（最大3枚のリファレンス画像 17、lastFrame指定によるトランジション生成 18）が必須。  
      * **ネイティブオーディオとリップシンク** 14 がキラーフィーチャーとなる。  
      * 既存のGoogle Cloud (Vertex AI) 16 エコシステム内でインフラを完結させたい。  
* **推奨3: クリエイター / 中小企業 / 迅速な試行錯誤（SaaS利用）**  
  * **ユースケース:** マーケティング素材、SNS投稿、VFXのコンセプトアートなど、プロフェッショナルまたはセミプロフェッショナルな品質の動画を、迅速なイテレーション（試行錯誤）で生成したい。  
  * **戦略:** APIの直接利用（秒単位課金）は、試行錯誤のコストが青天井になるため**非推奨**。予測可能な月額固定料金（サブスクリプション）のSaaSを選定する。  
  * **選定の分岐点:**  
    * **Adobe Firefly ($9.99/mo〜)** 31: 既にAdobe Creative Cloud（Premiere, Photoshop）を契約しているデザイナーやビデオ編集者。エコシステム内でのシームレスな連携 7 と商用利用の安全性 7 が最大の価値となる。  
    * **Runway ($28/mo)** 37: I+T2Vだけでなく、V2V、Inpainting、AI Magic Tools 43 など、動画編集プロセス全体をAIで高度化したいプロクリエイター。  
    * **Pika Labs ($8/mo〜)** 42: とにかくI+T2Vの制御性を追求したいパワーユーザーまたはホビイスト。-camera や \-motion 40 といったパラメータを駆使した精密な制御と、高いコストパフォーマンスを求める場合に最適。  
* **推奨4: 学術研究 / オープンソースベースの検証**  
  * **ユースケース:** 画像（I）から動画（V）を生成するI2Vの基本性能を、ローカル環境または自社サーバーで検証・研究したい。  
  * **最適ソリューション:** **Stable Video Diffusion (SVD)**  
  * **理由:** 現状、最も広く普及し、ベンチマークとして機能するオープン（ウェイト公開）モデルである 45。ただし、**テキストによる制御は不可能** 44 であり、ライセンスが**非商用** 45 であるという2つの重大な制約を厳密に理解した上で使用する必要がある。

### **8.2 開発者およびプロダクトマネージャーへの技術的推奨**

1. 「I+T2V」のアーキテクチャ的断絶を認識せよ:  
   オープンソース（SVD）と商用API（Sora/Veo）の間にあるギャップは、単なる「性能差」ではない（セクション5）。SVD（画像モデルのファインチューン）のアーキテクチャでは、テキストによる意味論的なモーション制御（I+T2V）は原理的に困難である 45。オープンソースベースでSora/Veoクローンを構築しようとする試みは、SVDの改良では達成できず、DiT/WFMクラスの大規模マルチモーダル基盤モデルの再構築が必要となり、莫大な訓練コストを伴うことを認識しなければならない。  
2. 「一貫性」こそが次世代の主戦場である:  
   動画生成の「品質（リアリズム）」は急速にコモディティ化しつつある。次の決定的な差別化要因、そして最大の技術的課題は「一貫性（Identity）」と「制御性（Motion）」である（セクション6）。プロダクトロードマップは、単に「より綺麗な動画」を目指すのではなく、ConsisID（周波数制御） 64 やS2V（ID/モーション分離） 61 の研究（セクション7）が示すような、「分離された制御モジュール」の実装による「意図した通りの動画」の生成を最優先すべきである。  
3. ビジネスモデルの二極化を戦略的に利用せよ:  
   AI動画市場は、「インフラ（API：秒課金）」と「アプリケーション（SaaS：月額課金）」に明確に分離している（セクション3, 4）。Sora APIの$0.30/s \[27\] とRunwayの$28/mo 37 の間には、巨大な価格差とビジネスモデルの裁定（アービトラージ）が存在する。自社のポジショニングがどちらにあるかを明確にし、APIコストの変動リスクをSaaSレイヤーで吸収する（あるいはSaaSとしてリスクを引き受ける）戦略を立てる必要がある。

### **8.3 最終結論**

「画像とテキストによる動画生成」という単一に見えた技術は、Nvidia Cosmosが示す「**産業シミュレーション（物理的正確性）**」と、Sora/Veoが示す「**クリエイティブ・メディア（視覚的説得力）**」という、二つの全く異なる未来へと分岐した。企業や開発者は、自らがどちらの未来を目指すのかを、まず決定しなければならない。

そして、どちらの未来においても、技術的な核心は「**いかにして参照（アイデンティティ）の一貫性を保ちながら、指示（モーション）を実行するか**」という、セクション6および7で詳述した根本的な課題に収斂する。この「維持 vs 変化」のトレードオフを、アーキテクチャレベルで「分離（Disentanglement）」し、ユーザーに制御可能なモジュールとして提供できたプレイヤーが、次の10年のAI動画生成市場を支配するだろう。現在のSoraやVeoのAPI仕様は、その長い戦いの始まりを告げる号砲に過ぎない。

#### **引用文献**

1. Physical AI with World Foundation Models | NVIDIA Cosmos, 11月 5, 2025にアクセス、 [https://www.nvidia.com/en-us/ai/cosmos/](https://www.nvidia.com/en-us/ai/cosmos/)  
2. nvidia/Cosmos-1.0-Diffusion-14B-Text2World · Hugging Face, 11月 5, 2025にアクセス、 [https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-14B-Text2World](https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-14B-Text2World)  
3. Physical AI and Robotics Conference Sessions | NVIDIA GTC 2025, 11月 5, 2025にアクセス、 [https://www.nvidia.com/gtc/sessions/physical-ai-and-robotics/](https://www.nvidia.com/gtc/sessions/physical-ai-and-robotics/)  
4. Veo \- Google DeepMind, 11月 5, 2025にアクセス、 [https://deepmind.google/models/veo/](https://deepmind.google/models/veo/)  
5. Meta Movie Gen, 11月 5, 2025にアクセス、 [https://ai.meta.com/research/movie-gen/](https://ai.meta.com/research/movie-gen/)  
6. Sora | OpenAI, 11月 5, 2025にアクセス、 [https://openai.com/sora/](https://openai.com/sora/)  
7. Image to video AI: Turn visuals into stunning videos. \- Adobe, 11月 5, 2025にアクセス、 [https://www.adobe.com/products/firefly/features/image-to-video.html](https://www.adobe.com/products/firefly/features/image-to-video.html)  
8. NVIDIA Launches Cosmos World Foundation Model Platform to Accelerate Physical AI Development, 11月 5, 2025にアクセス、 [https://nvidianews.nvidia.com/news/nvidia-launches-cosmos-world-foundation-model-platform-to-accelerate-physical-ai-development](https://nvidianews.nvidia.com/news/nvidia-launches-cosmos-world-foundation-model-platform-to-accelerate-physical-ai-development)  
9. A Survey: Spatiotemporal Consistency in Video Generation \- arXiv, 11月 5, 2025にアクセス、 [https://arxiv.org/html/2502.17863v1](https://arxiv.org/html/2502.17863v1)  
10. Cosmos for Autonomous Vehicles (AVs) and Robotics Developers ..., 11月 5, 2025にアクセス、 [https://developer.nvidia.com/cosmos](https://developer.nvidia.com/cosmos)  
11. NVIDIA Announces Major Release of Cosmos World Foundation Models and Physical AI Data Tools, 11月 5, 2025にアクセス、 [https://nvidianews.nvidia.com/news/nvidia-announces-major-release-of-cosmos-world-foundation-models-and-physical-ai-data-tools](https://nvidianews.nvidia.com/news/nvidia-announces-major-release-of-cosmos-world-foundation-models-and-physical-ai-data-tools)  
12. World Simulation With Video Foundation Models for Physical AI ..., 11月 5, 2025にアクセス、 [https://research.nvidia.com/publication/2025-09\_world-simulation-video-foundation-models-physical-ai](https://research.nvidia.com/publication/2025-09_world-simulation-video-foundation-models-physical-ai)  
13. NVIDIA Launches Open Models and Data to Accelerate AI Innovation Across Language, Biology and Robotics, 11月 5, 2025にアクセス、 [https://blogs.nvidia.com/blog/open-models-data-ai/](https://blogs.nvidia.com/blog/open-models-data-ai/)  
14. Veo 3 | Google AI Studio, 11月 5, 2025にアクセス、 [https://aistudio.google.com/models/veo-3](https://aistudio.google.com/models/veo-3)  
15. The 15 best AI video generators in 2025 \- Zapier, 11月 5, 2025にアクセス、 [https://zapier.com/blog/best-ai-video-generator/](https://zapier.com/blog/best-ai-video-generator/)  
16. Generate videos with Veo on Vertex AI from an image | Generative ..., 11月 5, 2025にアクセス、 [https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/generate-videos-from-an-image](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/generate-videos-from-an-image)  
17. Generate videos with Veo 3.1 in Gemini API | Google AI for Developers, 11月 5, 2025にアクセス、 [https://ai.google.dev/gemini-api/docs/video](https://ai.google.dev/gemini-api/docs/video)  
18. Ultimate prompting guide for Veo 3.1 | Google Cloud Blog, 11月 5, 2025にアクセス、 [https://cloud.google.com/blog/products/ai-machine-learning/ultimate-prompting-guide-for-veo-3-1](https://cloud.google.com/blog/products/ai-machine-learning/ultimate-prompting-guide-for-veo-3-1)  
19. Veo 3 preview | Generative AI on Vertex AI \- Google Cloud Documentation, 11月 5, 2025にアクセス、 [https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-0-generate-preview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-0-generate-preview)  
20. How Much does Veo 3 Cost? \- CometAPI \- All AI Models in One API, 11月 5, 2025にアクセス、 [https://www.cometapi.com/how-much-does-veo-3-cost-all-you-need-to-know/](https://www.cometapi.com/how-much-does-veo-3-cost-all-you-need-to-know/)  
21. Build with Veo 3, now available in the Gemini API \- Google Developers Blog, 11月 5, 2025にアクセス、 [https://developers.googleblog.com/en/veo-3-now-available-gemini-api/](https://developers.googleblog.com/en/veo-3-now-available-gemini-api/)  
22. Google's Veo 2 Video AI is accessible via API at $0.35/sec : r/Bard \- Reddit, 11月 5, 2025にアクセス、 [https://www.reddit.com/r/Bard/comments/1jvfpmg/googles\_veo\_2\_video\_ai\_is\_accessible\_via\_api\_at/](https://www.reddit.com/r/Bard/comments/1jvfpmg/googles_veo_2_video_ai_is_accessible_via_api_at/)  
23. Google's Veo 2 AI Video Model Priced at $0.50 per Second | MojoAuth, 11月 5, 2025にアクセス、 [https://mojoauth.com/blog/googles-new-ai-video-model-veo-2-will-cost-50-cents-per-second](https://mojoauth.com/blog/googles-new-ai-video-model-veo-2-will-cost-50-cents-per-second)  
24. Google Veo 2 video generation pricing: $30/minute : r/MediaSynthesis \- Reddit, 11月 5, 2025にアクセス、 [https://www.reddit.com/r/MediaSynthesis/comments/1ivy5ob/google\_veo\_2\_video\_generation\_pricing\_30minute/](https://www.reddit.com/r/MediaSynthesis/comments/1ivy5ob/google_veo_2_video_generation_pricing_30minute/)  
25. Sora video generation overview (preview) | Microsoft Learn, 11月 5, 2025にアクセス、 [https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/video-generation](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/video-generation)  
26. Video generation with Sora \- OpenAI API \- OpenAI Platform, 11月 5, 2025にアクセス、 [https://platform.openai.com/docs/guides/video-generation](https://platform.openai.com/docs/guides/video-generation)  
27. API Pricing \- OpenAI, 11月 5, 2025にアクセス、 [https://openai.com/api/pricing/](https://openai.com/api/pricing/)  
28. Cut Costs by 60% vs Official OpenAI Sora 2 API Pricing \- Kie.ai, 11月 5, 2025にアクセス、 [https://kie.ai/sora-2](https://kie.ai/sora-2)  
29. Generative credits FAQ \- Adobe Help Center, 11月 5, 2025にアクセス、 [https://helpx.adobe.com/creative-cloud/apps/generative-ai/generative-credits-faq.html](https://helpx.adobe.com/creative-cloud/apps/generative-ai/generative-credits-faq.html)  
30. Adobe Creative Cloud Plans, Pricing, and Membership, 11月 5, 2025にアクセス、 [https://www.adobe.com/creativecloud/plans.html](https://www.adobe.com/creativecloud/plans.html)  
31. Adobe Firefly \- Free Generative AI for creatives, 11月 5, 2025にアクセス、 [https://www.adobe.com/products/firefly.html](https://www.adobe.com/products/firefly.html)  
32. Compare plans that include generative AI | Adobe Firefly, 11月 5, 2025にアクセス、 [https://www.adobe.com/products/firefly/plans.html](https://www.adobe.com/products/firefly/plans.html)  
33. Text-to-Video: How to Write Prompts in Runway | by Rob Young ..., 11月 5, 2025にアクセス、 [https://medium.com/the-neuralist/text-to-video-how-to-write-prompts-in-runway-95d36c3b8159](https://medium.com/the-neuralist/text-to-video-how-to-write-prompts-in-runway-95d36c3b8159)  
34. Runway AI Video Generator \[Free Trial\] \- Monica, 11月 5, 2025にアクセス、 [https://monica.im/en/ai-models/runway](https://monica.im/en/ai-models/runway)  
35. Struggling with prompts \- GEN2 : r/runwayml \- Reddit, 11月 5, 2025にアクセス、 [https://www.reddit.com/r/runwayml/comments/14cotki/struggling\_with\_prompts\_gen2/](https://www.reddit.com/r/runwayml/comments/14cotki/struggling_with_prompts_gen2/)  
36. AI Image and Video Pricing from $12/month \- Runway, 11月 5, 2025にアクセス、 [https://runwayml.com/pricing](https://runwayml.com/pricing)  
37. Runway AI Pricing: Free vs Paid Plans, 11月 5, 2025にアクセス、 [https://www.imagine.art/blogs/runway-ai-pricing](https://www.imagine.art/blogs/runway-ai-pricing)  
38. Pikaffects by Pika \- App Store \- Apple, 11月 5, 2025にアクセス、 [https://apps.apple.com/us/app/pikaffects-by-pika/id6680155400](https://apps.apple.com/us/app/pikaffects-by-pika/id6680155400)  
39. Pika AI \[Free Trial\] \- Monica, 11月 5, 2025にアクセス、 [https://monica.im/en/ai-models/pika-ai](https://monica.im/en/ai-models/pika-ai)  
40. My Experiments with Pika Labs: Creating Videos from Text and ..., 11月 5, 2025にアクセス、 [https://kikiandmozart.beehiiv.com/p/pika-labs-early-experiments](https://kikiandmozart.beehiiv.com/p/pika-labs-early-experiments)  
41. Pika Labs Pricing, Features, Pros, Cons & 9 Best Alternatives \- DomoAI, 11月 5, 2025にアクセス、 [https://domoai.app/blog/pika-labs-pricing](https://domoai.app/blog/pika-labs-pricing)  
42. Pika Labs Pricing: Cost and Pricing plans \- SaaSworthy, 11月 5, 2025にアクセス、 [https://www.saasworthy.com/product/pika-labs/pricing](https://www.saasworthy.com/product/pika-labs/pricing)  
43. Runway Gen-2: AI Magic Tools for Effortless Content Creation | Deepgram, 11月 5, 2025にアクセス、 [https://deepgram.com/ai-apps/runway-gen-2](https://deepgram.com/ai-apps/runway-gen-2)  
44. Quickstart Guide to Stable Video Diffusion \- Civitai Education, 11月 5, 2025にアクセス、 [https://education.civitai.com/quickstart-guide-to-stable-video-diffusion/](https://education.civitai.com/quickstart-guide-to-stable-video-diffusion/)  
45. stabilityai/stable-video-diffusion-img2vid-xt · Hugging Face, 11月 5, 2025にアクセス、 [https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt)  
46. Image-to-Video Models \- Hugging Face, 11月 5, 2025にアクセス、 [https://huggingface.co/models?pipeline\_tag=image-to-video](https://huggingface.co/models?pipeline_tag=image-to-video)  
47. Ultimate Guide \- The Fastest Open Source Video Generation Models in 2025 \- SiliconFlow, 11月 5, 2025にアクセス、 [https://www.siliconflow.com/articles/en/fastest-open-source-video-generation-models](https://www.siliconflow.com/articles/en/fastest-open-source-video-generation-models)  
48. Ultimate Guide \- The Top Open Source AI Video Generation Models in 2025 \- SiliconFlow, 11月 5, 2025にアクセス、 [https://www.siliconflow.com/articles/en/best-open-source-models-for-video-to-text-transcription](https://www.siliconflow.com/articles/en/best-open-source-models-for-video-to-text-transcription)  
49. A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs \- arXiv, 11月 5, 2025にアクセス、 [https://arxiv.org/html/2403.07944v1](https://arxiv.org/html/2403.07944v1)  
50. A collection of awesome video generation studies. \- GitHub, 11月 5, 2025にアクセス、 [https://github.com/AlonzoLeeeooo/awesome-video-generation](https://github.com/AlonzoLeeeooo/awesome-video-generation)  
51. What is Text-to-Video? \- Hugging Face, 11月 5, 2025にアクセス、 [https://huggingface.co/tasks/text-to-video](https://huggingface.co/tasks/text-to-video)  
52. STIV: Scalable Text and Image Conditioned Video Generation \- Apple Machine Learning Research, 11月 5, 2025にアクセス、 [https://machinelearning.apple.com/research/conditioned-video-generation](https://machinelearning.apple.com/research/conditioned-video-generation)  
53. 10 Best AI Video Generators in 2025 – And How They Actually Work \- Retrocube, 11月 5, 2025にアクセス、 [https://www.retrocube.com/blog/10-best-ai-video-generators-in-2025-and-how-they-actually-work/](https://www.retrocube.com/blog/10-best-ai-video-generators-in-2025-and-how-they-actually-work/)  
54. Why do most AI image and video generators struggle with giving consistent results? \- Reddit, 11月 5, 2025にアクセス、 [https://www.reddit.com/r/generativeAI/comments/1mwlrpw/why\_do\_most\_ai\_image\_and\_video\_generators/](https://www.reddit.com/r/generativeAI/comments/1mwlrpw/why_do_most_ai_image_and_video_generators/)  
55. Technical Challenges to keep Character Consistency Across Image andVideo Generations, 11月 5, 2025にアクセス、 [https://dhirajpatra.medium.com/technical-challengges-to-keep-character-consistency-across-image-or-video-generations-071aee390e8e](https://dhirajpatra.medium.com/technical-challengges-to-keep-character-consistency-across-image-or-video-generations-071aee390e8e)  
56. What are the current challenges in AI video creation? : r/aiwars \- Reddit, 11月 5, 2025にアクセス、 [https://www.reddit.com/r/aiwars/comments/1get6jk/what\_are\_the\_current\_challenges\_in\_ai\_video/](https://www.reddit.com/r/aiwars/comments/1get6jk/what_are_the_current_challenges_in_ai_video/)  
57. What is Image-to-Video? \- Hugging Face, 11月 5, 2025にアクセス、 [https://huggingface.co/tasks/image-to-video](https://huggingface.co/tasks/image-to-video)  
58. ASurvey: Spatiotemporal Consistency in Video Generation | Request PDF \- ResearchGate, 11月 5, 2025にアクセス、 [https://www.researchgate.net/publication/389351274\_ASurvey\_Spatiotemporal\_Consistency\_in\_Video\_Generation](https://www.researchgate.net/publication/389351274_ASurvey_Spatiotemporal_Consistency_in_Video_Generation)  
59. \[2502.17863\] ASurvey: Spatiotemporal Consistency in Video Generation \- arXiv, 11月 5, 2025にアクセス、 [https://arxiv.org/abs/2502.17863](https://arxiv.org/abs/2502.17863)  
60. \[Literature Review\] ASurvey: Spatiotemporal Consistency in Video Generation, 11月 5, 2025にアクセス、 [https://www.themoonlight.io/en/review/asurvey-spatiotemporal-consistency-in-video-generation](https://www.themoonlight.io/en/review/asurvey-spatiotemporal-consistency-in-video-generation)  
61. Subject-driven Video Generation via Disentangled Identity and Motion, 11月 5, 2025にアクセス、 [https://carpedkm.github.io/projects/disentangled\_sub/](https://carpedkm.github.io/projects/disentangled_sub/)  
62. Identity-Preserving Text-to-Video Generation by Frequency Decomposition \- arXiv, 11月 5, 2025にアクセス、 [https://arxiv.org/html/2411.17440v3](https://arxiv.org/html/2411.17440v3)  
63. Identity-Preserving Text-to-Video Generation by Frequency Decomposition \- CVF Open Access, 11月 5, 2025にアクセス、 [https://openaccess.thecvf.com/content/CVPR2025/papers/Yuan\_Identity-Preserving\_Text-to-Video\_Generation\_by\_Frequency\_Decomposition\_CVPR\_2025\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition_CVPR_2025_paper.pdf)  
64. ConsisID: Identity-Preserving Text-to-Video Generation by Frequency Decomposition, 11月 5, 2025にアクセス、 [https://pku-yuangroup.github.io/ConsisID/](https://pku-yuangroup.github.io/ConsisID/)  
65. Identity-Preserving Text-to-Video Generation by Frequency Decomposition \- arXiv, 11月 5, 2025にアクセス、 [https://arxiv.org/html/2411.17440v1](https://arxiv.org/html/2411.17440v1)  
66. \[2411.17440\] Identity-Preserving Text-to-Video Generation by Frequency Decomposition \- arXiv, 11月 5, 2025にアクセス、 [https://arxiv.org/abs/2411.17440](https://arxiv.org/abs/2411.17440)  
67. Subject-driven Video Generation via Disentangled Identity and Motion \- arXiv, 11月 5, 2025にアクセス、 [https://arxiv.org/html/2504.17816v1](https://arxiv.org/html/2504.17816v1)
